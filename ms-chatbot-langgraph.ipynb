{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load environment variables\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the LLM Model and the Embedding model\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "llm=ChatOpenAI(model=\"gpt-4o-mini\",temperature=0)\n",
    "embeddings=OpenAIEmbeddings(model=\"text-embedding-3-small\", dimensions=1536)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the documents\n",
    "\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "loader_bsw = Docx2txtLoader(\"BSW SOM 0.1.docx\")\n",
    "loader_npac = Docx2txtLoader(\"NPAC COREX SOM v2.0.docx\")\n",
    "data_bsw = loader_bsw.load()\n",
    "data_npac = loader_npac.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import Any, List\n",
    "from langchain_text_splitters import TextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a custom splitter\n",
    "\n",
    "class GPTSplitter(TextSplitter):\n",
    "    def __init__(self, model_name: str = \"gpt-4o-mini\", **kwargs: Any) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.model = ChatOpenAI(model=model_name)\n",
    "\n",
    "        self.prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"You are an expert in analyzing and structuring technical documentation, particularly Service Operation Manuals (SOMs). Your task is to divide the given text into coherent, meaningful chunks that preserve both the document's structure and its detailed content. Each chunk should encapsulate a complete section or subsection, including all relevant details, explanations, and bullet points.\n",
    "\n",
    "                Follow these guidelines:\n",
    "                1. Preserve the hierarchical structure (e.g., main sections, subsections) and include all content under each header.\n",
    "                2. Keep related information together (e.g., lists, tables, contact information, detailed explanations).\n",
    "                3. Ensure each chunk is self-contained and includes all necessary context and details.\n",
    "                4. Aim for chunks that cover complete sections or logical groupings of subsections.\n",
    "                5. Include all text content, not just headers or bullet points.\n",
    "                6. Maintain the original formatting and structure within each chunk.\n",
    "\n",
    "                Wrap each chunk in <<<>>> markers.\n",
    "\n",
    "                Example:\n",
    "                <<<3. Centiq Managed Service\n",
    "                3.1. Introduction\n",
    "                • Centiq provides comprehensive managed services for SAP environments\n",
    "                • Our team of experts ensures 24/7 monitoring and support\n",
    "                • We offer proactive maintenance and optimization\n",
    "                [Include all bullet points and any additional explanatory text here]\n",
    "\n",
    "                3.2. Managed Service Description\n",
    "                3.2.1. Service Operations\n",
    "                • 24/7 monitoring of SAP systems and infrastructure\n",
    "                • Incident management and resolution\n",
    "                • Performance tuning and optimization\n",
    "                [Include all bullet points and detailed descriptions of each service operation]>>>\n",
    "\n",
    "                Now, process the following text from the Service Operation Manual, ensuring to include ALL content and details:\n",
    "\n",
    "                {text}\"\"\"\n",
    "        )\n",
    "        self.output_parser = StrOutputParser()\n",
    "        self.chain = (\n",
    "            {\"text\": RunnablePassthrough()}\n",
    "            | self.prompt\n",
    "            | self.model\n",
    "            | self.output_parser\n",
    "        )\n",
    "\n",
    "    # def split_text(self, text: str) -> List[str]:\n",
    "    #     response = self.chain.invoke({\"text\": text})\n",
    "    #     # Use regex to split properly by <<< and >>> markers\n",
    "    #     chunks = re.findall(r'<<<(.*?)>>>', response, re.DOTALL)\n",
    "    #     return [chunk.strip() for chunk in chunks]\n",
    "    \n",
    "    def split_text(self, text: str) -> List[Document]:\n",
    "      chunks = self.chain.invoke(text).split('<<<')[1:]  # Split the result and remove the first empty element\n",
    "      return [Document(page_content=chunk.strip('>>>').strip()) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the documents using the custom splitter\n",
    "\n",
    "gpt_splitter = GPTSplitter()\n",
    "bsw_docs = gpt_splitter.split_text(data_bsw)\n",
    "npac_docs = gpt_splitter.split_text(data_npac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in bsw_docs:\n",
    "    doc.metadata[\"source\"] = \"BSW\"\n",
    "\n",
    "for doc in npac_docs:\n",
    "    doc.metadata[\"source\"] = \"NPAC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunks = bsw_docs + npac_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x17b23a9b050>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Chroma vector store\n",
    "\n",
    "Chroma.from_documents(\n",
    "    documents=all_chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./SOM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading the vector database from local db\n",
    "\n",
    "vectordb=Chroma(persist_directory=\"SOM\",embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vectordb.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating the multiquery retriever\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "import re\n",
    "\n",
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an expert query rewriter specializing in SAP Basis managed services. Your task is to take an original query and generate at least upto different versions of the same query. These variations should:\n",
    "\n",
    "                1. Maintain the core intent of the original query\n",
    "                2. Use different phrasings, synonyms, and technical terms\n",
    "                3. Vary in complexity, from simple to more detailed\n",
    "                4. Include relevant SAP Basis-specific terminology where appropriate\n",
    "                5. Consider different aspects or perspectives of the original question\n",
    "                6. Utilize common SAP Basis synonyms and alternative terms where possible\n",
    "                7. Consider using alternate terms where appropriate, e.g system refresh for system copy, etc\n",
    "\n",
    "                Please provide upto 3 rewritten versions of this query, ensuring that each version is distinct and adds value to the search process. Label each rewritten query clearly.\n",
    "\n",
    "                Provide these alternative question like this:\n",
    "                    <<question1>>\n",
    "                    <<question2>>\n",
    "                Only provide the query, no numbering.\n",
    "\n",
    "                Remember to consider various user roles (e.g., SAP Basis administrators, support staff, managers) and their potential ways of asking the same question.\n",
    "\n",
    "                Original question: {question}\"\"\",\n",
    ")\n",
    "\n",
    "\n",
    "def split_and_clean_text(input_text):\n",
    "    return [item for item in re.split(r\"<<|>>\", input_text) if item.strip()]\n",
    "\n",
    "multiquery_chain = (\n",
    "    QUERY_PROMPT | llm | StrOutputParser() | RunnableLambda(split_and_clean_text)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a function for returning unique documents(remove duplicate docs)\n",
    "\n",
    "def flatten_and_unique_documents(documents):\n",
    "    flattened_docs = [doc for sublist in documents for doc in sublist]\n",
    "\n",
    "    unique_docs = []\n",
    "    unique_contents = set()\n",
    "    for doc in flattened_docs:\n",
    "        if doc.page_content not in unique_contents:\n",
    "            unique_docs.append(doc)\n",
    "            unique_contents.add(doc.page_content)\n",
    "\n",
    "    return unique_docs\n",
    "\n",
    "def find_rel_docs(query):\n",
    "    list_of_questions = multiquery_chain.invoke(query)\n",
    "    docs = [retriever.invoke(q) for q in list_of_questions]\n",
    "    final_docs=flatten_and_unique_documents(documents=docs)\n",
    "    return final_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_chain=multiquery_chain | RunnableLambda(find_rel_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate an answer\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain import hub\n",
    "\n",
    "answer_template = \"\"\"You are an AI assistant tasked with answering questions based on the provided documents. Your goal is to generate accurate and informative answers using only the information contained in these documents.\n",
    "\n",
    "When answering, please follow these guidelines:\n",
    "1. Analyze the given question and the provided context documents carefully.\n",
    "2. Formulate your answer in a clear, concise, and structured manner.\n",
    "3. Present your answer in a point-wise format, using numbered points for main ideas or steps.\n",
    "4. Use sub-points (a, b, c) if necessary to provide additional details under a main point.\n",
    "5. Ensure each point is directly relevant to the question and supported by the context documents.\n",
    "6. If the information in the documents is insufficient to fully answer the question, state this clearly and provide any partial information that is available.\n",
    "7. Do not contradict yourself or provide inconsistent information.\n",
    "\n",
    "Question: {question} \n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "answer_prompt = ChatPromptTemplate.from_template(answer_template)\n",
    "\n",
    "llm=ChatOpenAI(model=\"gpt-4o-mini\",temperature=0.1)\n",
    "\n",
    "def format_docs(relevant_docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in relevant_docs)\n",
    "\n",
    "\n",
    "\n",
    "answer_chain = (\n",
    "    {\"context\":retrieve_chain | format_docs, \"question\": RunnablePassthrough()} # type: ignore\n",
    "    | answer_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        list_of_questions: list of questions\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    list_of_questions: List[str]\n",
    "    answer: str\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "import time\n",
    "\n",
    "\n",
    "def query_rewriter(state):\n",
    "    \"\"\"\n",
    "    Generate 5 alternate queries\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, list of questions\n",
    "    \"\"\"\n",
    "    print(\"---GENERATING ALTERNATE QUERIES---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Generating questions\n",
    "    list_of_questions = multiquery_chain.invoke(question)\n",
    "    for query in list_of_questions:\n",
    "        print(f\"- {query}\")\n",
    "        time.sleep(0.5)\n",
    "    return {\"list_of_questions\": list_of_questions}\n",
    "\n",
    "\n",
    "# def retrieve_docs(state):\n",
    "#     \"\"\"\n",
    "#     Return unique documents that matches the context of the question asked\n",
    "\n",
    "#     Args:\n",
    "#         state (dict): The current graph state\n",
    "\n",
    "#     Returns:\n",
    "#         state (dict): New key added to state, retrieved documents\n",
    "#     \"\"\"\n",
    "#     print(\"---GETTING DOCUMENTS---\")\n",
    "#     list_of_questions = state[\"list_of_questions\"]\n",
    "#     docs = [retriever.invoke(q) for q in list_of_questions]\n",
    "#     final_docs=flatten_and_unique_documents(documents=docs)\n",
    "    \n",
    "#     return {\"documents\": final_docs}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generates an answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Generates the final answer\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---GENERATING ANSWER---\")\n",
    "\n",
    "    documents = state[\"documents\"]\n",
    "    query = state[\"question\"]\n",
    "    answer = answer_chain.invoke({\"question\":query})\n",
    "    return {\"answer\":answer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"query_rewriter\", query_rewriter)  # rewrite query\n",
    "# workflow.add_node(\"retrieve\", retrieve_docs)  # retrieve documents\n",
    "workflow.add_node(\"generate\", generate)  # generate answer\n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"query_rewriter\")    \n",
    "#workflow.add_edge(\"query_rewriter\", \"retrieve\")\n",
    "#workflow.add_edge(\"retrieve\", \"generate\")\n",
    "workflow.add_edge(\"query_rewriter\", \"generate\")\n",
    "workflow.add_edge(\"generate\",END)\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Service Operations Manual Chatbot!\n",
      "Ask questions about BSW or NPAC, or type 'quit' to exit.\n",
      "Thinking...\n",
      "---GENERATING ALTERNATE QUERIES---\n",
      "- What is the total number of system copies included in the scope for BSW?\n",
      "- Can you provide the count of system refreshes that fall under the BSW scope?\n",
      "- How many system clones are accounted for within the BSW parameters?\n",
      "Node 'query_rewriter':\n",
      "\n",
      "---\n",
      "\n",
      "---GENERATING ANSWER---\n",
      "Node 'generate':\n",
      "\n",
      "---\n",
      "\n",
      "Based on the provided context documents, the information regarding the number of system copies in scope for BSW is not explicitly stated. However, I can summarize the relevant details:\n",
      "\n",
      "1. **System Management Activities**:\n",
      "   - The documents mention activities related to SAP Basis operations, which include managing landscape diagrams and client strategies, such as local client creation and copies.\n",
      "   - Specifically, it notes that \"local client copies\" are scheduled as required, indicating that there is a process for managing system copies.\n",
      "\n",
      "2. **System Refresh**:\n",
      "   - There is a mention of a \"System Refresh (P -> Q copy)\" which is excluded from Business As Usual (BAU) activities and requires scoping and pricing. This implies that system copies are part of the operational scope but are not defined in terms of quantity.\n",
      "\n",
      "3. **Lack of Specific Numbers**:\n",
      "   - The documents do not provide a specific number of system copies that are in scope for BSW. The details regarding the exact number of copies or systems managed are not included in the provided context.\n",
      "\n",
      "In conclusion, while the documents indicate that system copies are part of the operational activities, they do not specify how many system copies are in scope for BSW. Further clarification from BSW or additional documentation may be required to obtain this specific information.\n",
      "Thank you for using the chatbot. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import uuid\n",
    "\n",
    "def generate_session_id():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "def chatbot():\n",
    "   \n",
    "    print(\"Welcome to the Service Operations Manual Chatbot!\")\n",
    "    print(\"Ask questions about BSW or NPAC, or type 'quit' to exit.\")\n",
    "\n",
    "    # Generate a new session ID for this conversation\n",
    "    session_id = generate_session_id()\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"\\nYour question: \").strip()\n",
    "        \n",
    "        if query.lower() == 'quit':\n",
    "            print(\"Thank you for using the chatbot. Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if not query:\n",
    "            print(\"Please enter a valid question.\")\n",
    "            continue\n",
    "        \n",
    "        print(\"Thinking...\")\n",
    "        \n",
    "        try:\n",
    "\n",
    "            # Use the session_id in the config\n",
    "            config = {\"configurable\": {\"thread_id\": session_id}}\n",
    "\n",
    "            for output in app.stream({\"question\":query}, config):\n",
    "                for key, value in output.items():\n",
    "                    # Node\n",
    "                    print(f\"Node '{key}':\")\n",
    "                    # Optional: print full state at each node\n",
    "                    # pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "                print(\"\\n---\\n\")\n",
    "\n",
    "            # Final generation\n",
    "            print(value[\"answer\"])\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {str(e)}\")\n",
    "            print(\"Please try asking your question in a different way.\")\n",
    "\n",
    "# Run the chatbot\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knaiml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
